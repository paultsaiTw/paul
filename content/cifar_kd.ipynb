{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piAVeOax6ph6"
      },
      "source": [
        "# Distilling knowledge in models pretrained on CIFAR-10/100 datasets, using ***torchdistill***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqZF7kY-6zDh"
      },
      "source": [
        "## 1. Make sure you have access to GPU/TPU\n",
        "Google Colab: *Runtime* -> *Change runtime type* -> *Hardware accelarator*: \"GPU\" or \"TPU\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "QjtzUsDm8JDY",
        "outputId": "8371aafe-29df-4408-a2c0-67f3a0ee6dd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy7V9yQH6o3J",
        "outputId": "76457d53-3dde-4ef3-fb5d-ccad12613b6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jul 16 02:41:30 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur2bTpuW64r5"
      },
      "source": [
        "## 2. Install ***torchdistill***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJSCmbJH_yuC",
        "outputId": "266ec0e9-3e55-4eff-bd3b-e7be1cc51dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchdistill\n",
            "  Downloading torchdistill-1.1.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: torch>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from torchdistill) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.20.1 in /usr/local/lib/python3.11/dist-packages (from torchdistill) (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchdistill) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.11/dist-packages (from torchdistill) (6.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torchdistill) (1.15.3)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (from torchdistill) (3.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->torchdistill) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->torchdistill) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->torchdistill) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->torchdistill) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->torchdistill) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.5.1->torchdistill)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.5.1->torchdistill)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.5.1->torchdistill)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.5.1->torchdistill)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.5.1->torchdistill)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.5.1->torchdistill)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.5.1->torchdistill)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.5.1->torchdistill)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.5.1->torchdistill)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->torchdistill) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->torchdistill) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->torchdistill) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.5.1->torchdistill)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->torchdistill) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->torchdistill) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.5.1->torchdistill) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.20.1->torchdistill) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.5.1->torchdistill) (3.0.2)\n",
            "Downloading torchdistill-1.1.3-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.0/89.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchdistill\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n"
          ]
        }
      ],
      "source": [
        "!pip install torchdistill"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOKnpfrD7DFC"
      },
      "source": [
        "## 3. Clone ***torchdistill*** repository to use its example code and configuration files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa4hqLn57Hkz",
        "outputId": "e00f01fe-16e1-487e-9299-c2629b9d8966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'torchdistill'...\n",
            "remote: Enumerating objects: 11067, done.\u001b[K\n",
            "remote: Counting objects: 100% (2030/2030), done.\u001b[K\n",
            "remote: Compressing objects: 100% (516/516), done.\u001b[K\n",
            "remote: Total 11067 (delta 1596), reused 1532 (delta 1514), pack-reused 9037 (from 5)\u001b[K\n",
            "Receiving objects: 100% (11067/11067), 10.81 MiB | 27.88 MiB/s, done.\n",
            "Resolving deltas: 100% (6778/6778), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/yoshitomo-matsubara/torchdistill.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGrp0OmZ7a_u"
      },
      "source": [
        "## 4. Distill knowledge in models pretrained on CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VETfswVJ7Xj0"
      },
      "source": [
        "Note that the hyperparameters of ResNet, WRN (Wide ResNet), and DenseNet-BC were chosen based on either train/val (splitting 50k samples into train:val = 45k:5k) or cross-validation, according to the original papers.  \n",
        "For the final run (once the hyperparameters are finalized), the authors used all the training images (50k samples).  \n",
        "- ResNet: https://github.com/facebookarchive/fb.resnet.torch\n",
        "- WRN (Wide ResNet): https://github.com/szagoruyko/wide-residual-networks\n",
        "- DenseNet-BC: https://github.com/liuzhuang13/DenseNet\n",
        "\n",
        "The following examples demonstrate how to 1) tune hyperparameter and 2) do final-run with ResNet-20 on CIFAR-10 dataset, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TpYnaQs781n"
      },
      "source": [
        "### 4.1 Hyperparameter tuning based on train:val = 45k:5k\n",
        "Let's start with a small **student model**, ResNet-20, with a pretrained DenseNet-BC (k=12, depth=100) as a **teacher model** for tutorial.  \n",
        "\n",
        "Open `torchdistill/configs/sample/cifar10/kd/resnet20_from_densenet_bc_k12_depth100-hyperparameter_tuning.yaml` and update hyperparameters as you wish e.g., number of epochs (*num_epochs*), batch size (*batch_size* in *train_data_loader* entry), learning rate (*lr* within *optimizer* entry), and so on.\n",
        "By default, the hyperparameters in the example config are identical to those in the final run config.\n",
        "  \n",
        "You will find a lot of module names from [PyTorch documentation](https://pytorch.org/docs/stable/index.html) and [torchvision](https://pytorch.org/docs/stable/torchvision/) such as [`SGD`](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD), [`MultiStepLR`](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.MultiStepLR), [`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss), [`CIFAR10`](https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.CIFAR10), [`RandomCrop`](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomCrop) (, and more). You can update their parameters or replace such modules with other modules in the packages. For instance, `SGD` could be replaced with [`Adam`](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam), and then you will change the parameters under `params` (at least delete `momentum` entry as the parameter is not for `Adam`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy9Yr2tB8avA",
        "outputId": "d23f4297-32ee-4ebf-d374-220e5d7f8a24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/torchdistill/examples/image_classification.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python torchdistill/examples/image_classification.py --config torchdistill/configs/sample/cifar10/kd/resnet20_from_densenet_bc_k12_depth100-hyperparameter_tuning.yaml --run_log log/cifar10/kd/resnet20_from_densenet_bc_k12_depth100-hyperparameter_tuning.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t8BdOEI8kHJ"
      },
      "source": [
        "### 4.2 Final run with hyperparameters determinded by the above hyperparameter-tuning\n",
        "Once you tune the hyperparameters, you can update the values in **a config file whose name ends with \"-final_run.yaml\"**. Notice that the only difference between default example configs for hyperparameter tuning and final run is datasets entry."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/yoshitomo-matsubara/torchdistill.git\n",
        "%cd torchdistill"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANWFH2VPHmqV",
        "outputId": "678a458e-c54b-4eff-cff4-ae48ac27584b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'torchdistill' already exists and is not an empty directory.\n",
            "/content/torchdistill\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6EQTNz3LqGm",
        "outputId": "de6b18b4-45f7-4e2f-e5b6-afd884699a8e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/torchdistill\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "print(\"CIFAR-10 train set size:\", len(dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7u9qzWSQQdR",
        "outputId": "66c80974-acac-4ac1-f763-d6282d155f09"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 76.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFAR-10 train set size: 50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat configs/sample/cifar10/kd/resnet20_from_resnet56.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyMFZSW8Qr8n",
        "outputId": "ba4f0ced-affa-41fe-c5af-7d33ab58b17a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: configs/sample/cifar10/kd/resnet20_from_resnet56.yaml: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlkskjjK8kxC",
        "outputId": "41456ef4-f283-4109-8c46-118addae21b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025/07/16 03:29:06\tINFO\ttorchdistill.common.main_util\tNot using distributed mode\n",
            "2025/07/16 03:29:06\tINFO\t__main__\tNamespace(config='/content/torchdistill/configs/sample/cifar10/kd/resnet20_from_densenet_bc_k12_depth100-final_run.yaml', device='cuda', run_log='/content/torchdistill/log/cifar10/kd/resnet20_from_densenet_bc_k12_depth100-final_run.log', start_epoch=0, seed=None, disable_cudnn_benchmark=False, test_only=False, student_only=False, log_config=False, world_size=1, dist_url='env://', adjust_lr=False)\n",
            "2025/07/16 03:29:06\tINFO\ttorchdistill.common.main_util\tGetting `RandomSampler` from `torch.utils.data`\n",
            "2025/07/16 03:29:06\tINFO\ttorchdistill.common.main_util\tGetting `SequentialSampler` from `torch.utils.data`\n",
            "2025/07/16 03:29:06\tINFO\ttorchdistill.common.main_util\tckpt file path is None\n",
            "2025/07/16 03:29:06\tINFO\ttorchdistill.common.main_util\tckpt file path is None\n",
            "2025/07/16 03:29:06\tINFO\t__main__\tStart training\n",
            "2025/07/16 03:29:06\tINFO\ttorchdistill.core.distillation\t[teacher model]\n",
            "2025/07/16 03:29:06\tINFO\ttorchdistill.models.util\tUsing the original model\n",
            "2025/07/16 03:29:06\tINFO\ttorchdistill.core.distillation\t[student model]\n",
            "2025/07/16 03:29:06\tINFO\ttorchdistill.models.util\tUsing the original model\n",
            "2025/07/16 03:29:06\tINFO\ttorchdistill.core.distillation\tLoss = 1.0 * KDLoss(\n",
            "  (cross_entropy_loss): CrossEntropyLoss()\n",
            ")\n",
            "2025/07/16 03:29:06\tINFO\ttorchdistill.core.distillation\tFreezing the whole teacher model\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/torchdistill/examples/torchvision/image_classification.py\", line 211, in <module>\n",
            "    main(argparser.parse_args())\n",
            "  File \"/content/torchdistill/examples/torchvision/image_classification.py\", line 189, in main\n",
            "    train(teacher_model, student_model, dataset_dict, src_ckpt_file_path, dst_ckpt_file_path,\n",
            "  File \"/content/torchdistill/examples/torchvision/image_classification.py\", line 140, in train\n",
            "    train_one_epoch(training_box, device, epoch, log_freq)\n",
            "  File \"/content/torchdistill/examples/torchvision/image_classification.py\", line 61, in train_one_epoch\n",
            "    for sample_batch, targets, supp_dict in \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchdistill/misc/log.py\", line 203, in log_every\n",
            "    space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
            "                                  ^^^^^^^^^^^^^\n",
            "TypeError: object of type 'NoneType' has no len()\n"
          ]
        }
      ],
      "source": [
        "!python /content/torchdistill/examples/torchvision/image_classification.py --config /content/torchdistill/configs/sample/cifar10/kd/resnet20_from_densenet_bc_k12_depth100-final_run.yaml --run_log /content/torchdistill/log/cifar10/kd/resnet20_from_densenet_bc_k12_depth100-final_run.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUTCWU7PGB31"
      },
      "source": [
        "At the end of the training process, you will see improved accuracy of the student model (ResNet-20) compared to that trained without teacher in another example notebook and/or the accuracy reported in [the ResNet paper (Table 6)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H04cv_LK833s"
      },
      "source": [
        "## 5. More sample configurations, models, datasets...\n",
        "For CIFAR-10/100 datasets, you can find more [sample configurations](https://github.com/yoshitomo-matsubara/torchdistill/tree/master/configs/legacy/sample/) and [models](https://github.com/yoshitomo-matsubara/torchdistill/tree/master/torchdistill/models/classification) in the [***torchdistill***](https://github.com/yoshitomo-matsubara/torchdistill) repository.\n",
        "If you would like to use larger datasets e.g., **ImageNet** and **COCO** datasets and models in `torchvision` (or your own modules), refer to the [official configurations](https://github.com/yoshitomo-matsubara/torchdistill/tree/master/configs/legacy/official) used in some published papers.\n",
        "Experiments with such large datasets and models will require you to use your own machine due to limited disk space and session time (12 hours for free version and 24 hours for Colab Pro) on Google Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuzcVbp99EDb"
      },
      "source": [
        "# Colab examples for training student models without teacher models\n",
        "You can find Colab examples for training models without teachers in the [***torchdistill***](https://github.com/yoshitomo-matsubara/torchdistill) repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d3c912e",
        "outputId": "d97d0fb2-848a-44d9-a1e1-73c4328252fd"
      },
      "source": [
        "!cat <<EOF > /content/torchdistill/configs/sample/cifar10/kd/resnet20_from_densenet_bc_k12_depth100-final_run.yaml\n",
        "datasets:\n",
        "  cifar10:\n",
        "    name: &dataset_name 'cifar10'\n",
        "    key: 'CIFAR10'\n",
        "    root: &root_dir !join ['./resource/dataset/', *dataset_name]\n",
        "    splits:\n",
        "      train:\n",
        "        dataset_id: &cifar10_train !join [*dataset_name, '/train']\n",
        "        kwargs:\n",
        "          root: *root_dir\n",
        "          train: True\n",
        "          download: True\n",
        "          transform_configs:\n",
        "            - key: 'RandomCrop'\n",
        "              kwargs:\n",
        "                size: 32\n",
        "                padding: 4\n",
        "            - key: 'RandomHorizontalFlip'\n",
        "              kwargs:\n",
        "                p: 0.5\n",
        "            - key: 'ToTensor'\n",
        "              kwargs:\n",
        "            - &normalize\n",
        "              key: 'Normalize'\n",
        "              kwargs:\n",
        "                mean: [0.49139968, 0.48215841, 0.44653091]\n",
        "                std: [0.24703223, 0.24348513, 0.26158784]\n",
        "      val:\n",
        "        dataset_id: &cifar10_val !join [ *dataset_name, '/val' ]\n",
        "        kwargs:\n",
        "          root: *root_dir\n",
        "          train: False\n",
        "          download: True\n",
        "          transform_configs: &val_transform\n",
        "            - key: 'ToTensor'\n",
        "              kwargs:\n",
        "            - *normalize\n",
        "      test:\n",
        "        dataset_id: &cifar10_test !join [*dataset_name, '/test']\n",
        "        kwargs:\n",
        "          root: *root_dir\n",
        "          train: False\n",
        "          download: True\n",
        "          transform_configs: *val_transform\n",
        "\n",
        "models:\n",
        "  teacher_model:\n",
        "    key: &teacher_model_key 'densenet_bc_k12_depth100'\n",
        "    kwargs:\n",
        "      num_classes: 10\n",
        "      memory_efficient: False\n",
        "      pretrained: True\n",
        "    src_ckpt:\n",
        "  student_model:\n",
        "    key: &student_model_key 'resnet20'\n",
        "    kwargs:\n",
        "      num_classes: 10\n",
        "      pretrained: False\n",
        "    _experiment: &student_experiment !join [*dataset_name, '-', *student_model_key, '_from_', *teacher_model_key]\n",
        "    src_ckpt:\n",
        "    dst_ckpt: !join ['./resource/ckpt/', *dataset_name, '/kd/', *student_experiment, '-final_run.pt']\n",
        "\n",
        "train:\n",
        "  log_freq: 100\n",
        "  num_epochs: 182\n",
        "  train_data_loader:\n",
        "    dataset_id: *cifar10_train\n",
        "    sampler:\n",
        "      class_or_func: !import_get\n",
        "        key: 'torch.utils.data.RandomSampler'\n",
        "      kwargs:\n",
        "    kwargs:\n",
        "      batch_size: 64\n",
        "      num_workers: 16\n",
        "      pin_memory: True\n",
        "      drop_last: False\n",
        "    cache_output:\n",
        "  val_data_loader:\n",
        "    dataset_id: *cifar10_val # Changed to use the val dataset split\n",
        "    sampler: &val_sampler\n",
        "      class_or_func: !import_get\n",
        "        key: 'torch.utils.data.SequentialSampler'\n",
        "      kwargs:\n",
        "    kwargs:\n",
        "      batch_size: 128\n",
        "      num_workers: 16\n",
        "      pin_memory: True\n",
        "      drop_last: False\n",
        "  teacher:\n",
        "    forward_proc: 'forward_batch_only'\n",
        "    sequential: []\n",
        "    wrapper: 'DataParallel'\n",
        "    requires_grad: False\n",
        "    frozen_modules: []\n",
        "  student:\n",
        "    forward_proc: 'forward_batch_only'\n",
        "    adaptations:\n",
        "    sequential: []\n",
        "    wrapper: 'DistributedDataParallel'\n",
        "    requires_grad: True\n",
        "    frozen_modules: []\n",
        "  optimizer:\n",
        "    key: 'SGD'\n",
        "    kwargs:\n",
        "      lr: 0.1\n",
        "      momentum: 0.9\n",
        "      weight_decay: 0.0001\n",
        "  scheduler:\n",
        "    key: 'MultiStepLR'\n",
        "    kwargs:\n",
        "      milestones: [91, 136]\n",
        "      gamma: 0.1\n",
        "  criterion:\n",
        "    key: 'WeightedSumLoss'\n",
        "    kwargs:\n",
        "      sub_terms:\n",
        "        kd:\n",
        "          criterion:\n",
        "            key: 'KDLoss'\n",
        "            kwargs:\n",
        "              student_module_path: '.'\n",
        "              student_module_io: 'output'\n",
        "              teacher_module_path: '.'\n",
        "              teacher_module_io: 'output'\n",
        "              temperature: 4.0\n",
        "              alpha: 0.9\n",
        "              reduction: 'batchmean'\n",
        "          weight: 1.0\n",
        "\n",
        "test:\n",
        "  test_data_loader: # Added test_data_loader\n",
        "    dataset_id: *cifar10_test\n",
        "    sampler: *val_sampler\n",
        "    kwargs:\n",
        "      batch_size: 1\n",
        "      num_workers: 16\n",
        "      pin_memory: True\n",
        "      drop_last: False\n",
        "EOF"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets:\n",
            "  cifar10:\n",
            "    name: &dataset_name 'cifar10'\n",
            "    key: 'CIFAR10'\n",
            "    root: &root_dir !join ['./resource/dataset/', *dataset_name]\n",
            "    splits:\n",
            "      train:\n",
            "        dataset_id: &cifar10_train !join [*dataset_name, '/train']\n",
            "        kwargs:\n",
            "          root: *root_dir\n",
            "          train: True\n",
            "          download: True\n",
            "          transform_configs:\n",
            "            - key: 'RandomCrop'\n",
            "              kwargs:\n",
            "                size: 32\n",
            "                padding: 4\n",
            "            - key: 'RandomHorizontalFlip'\n",
            "              kwargs:\n",
            "                p: 0.5\n",
            "            - key: 'ToTensor'\n",
            "              kwargs:\n",
            "            - &normalize\n",
            "              key: 'Normalize'\n",
            "              kwargs:\n",
            "                mean: [0.49139968, 0.48215841, 0.44653091]\n",
            "                std: [0.24703223, 0.24348513, 0.26158784]\n",
            "      val:\n",
            "        dataset_id: &cifar10_val !join [ *dataset_name, '/val' ]\n",
            "        kwargs:\n",
            "          root: *root_dir\n",
            "          train: False\n",
            "          download: True\n",
            "          transform_configs: &val_transform\n",
            "            - key: 'ToTensor'\n",
            "              kwargs:\n",
            "            - *normalize\n",
            "      test:\n",
            "        dataset_id: &cifar10_test !join [*dataset_name, '/test']\n",
            "        kwargs:\n",
            "          root: *root_dir\n",
            "          train: False\n",
            "          download: True\n",
            "          transform_configs: *val_transform\n",
            "\n",
            "models:\n",
            "  teacher_model:\n",
            "    key: &teacher_model_key 'densenet_bc_k12_depth100'\n",
            "    kwargs:\n",
            "      num_classes: 10\n",
            "      memory_efficient: False\n",
            "      pretrained: True\n",
            "    src_ckpt:\n",
            "  student_model:\n",
            "    key: &student_model_key 'resnet20'\n",
            "    kwargs:\n",
            "      num_classes: 10\n",
            "      pretrained: False\n",
            "    _experiment: &student_experiment !join [*dataset_name, '-', *student_model_key, '_from_', *teacher_model_key]\n",
            "    src_ckpt:\n",
            "    dst_ckpt: !join ['./resource/ckpt/', *dataset_name, '/kd/', *student_experiment, '-final_run.pt']\n",
            "\n",
            "train:\n",
            "  log_freq: 100\n",
            "  num_epochs: 182\n",
            "  train_data_loader:\n",
            "    dataset_id: *cifar10_train\n",
            "    sampler:\n",
            "      class_or_func: !import_get\n",
            "        key: 'torch.utils.data.RandomSampler'\n",
            "      kwargs:\n",
            "    kwargs:\n",
            "      batch_size: 64\n",
            "      num_workers: 16\n",
            "      pin_memory: True\n",
            "      drop_last: False\n",
            "    cache_output:\n",
            "  val_data_loader:\n",
            "    dataset_id: *cifar10_test\n",
            "    sampler: &val_sampler\n",
            "      class_or_func: !import_get\n",
            "        key: 'torch.utils.data.SequentialSampler'\n",
            "      kwargs:\n",
            "    kwargs:\n",
            "      batch_size: 128\n",
            "      num_workers: 16\n",
            "      pin_memory: True\n",
            "      drop_last: False\n",
            "  teacher:\n",
            "    forward_proc: 'forward_batch_only'\n",
            "    sequential: []\n",
            "    wrapper: 'DataParallel'\n",
            "    requires_grad: False\n",
            "    frozen_modules: []\n",
            "  student:\n",
            "    forward_proc: 'forward_batch_only'\n",
            "    adaptations:\n",
            "    sequential: []\n",
            "    wrapper: 'DistributedDataParallel'\n",
            "    requires_grad: True\n",
            "    frozen_modules: []\n",
            "  optimizer:\n",
            "    key: 'SGD'\n",
            "    kwargs:\n",
            "      lr: 0.1\n",
            "      momentum: 0.9\n",
            "      weight_decay: 0.0001\n",
            "  scheduler:\n",
            "    key: 'MultiStepLR'\n",
            "    kwargs:\n",
            "      milestones: [91, 136]\n",
            "      gamma: 0.1\n",
            "  criterion:\n",
            "    key: 'WeightedSumLoss'\n",
            "    kwargs:\n",
            "      sub_terms:\n",
            "        kd:\n",
            "          criterion:\n",
            "            key: 'KDLoss'\n",
            "            kwargs:\n",
            "              student_module_path: '.'\n",
            "              student_module_io: 'output'\n",
            "              teacher_module_path: '.'\n",
            "              teacher_module_io: 'output'\n",
            "              temperature: 4.0\n",
            "              alpha: 0.9\n",
            "              reduction: 'batchmean'\n",
            "          weight: 1.0\n",
            "\n",
            "test:\n",
            "  test_data_loader:\n",
            "    dataset_id: *cifar10_test\n",
            "    sampler: *val_sampler\n",
            "    kwargs:\n",
            "      batch_size: 1\n",
            "      num_workers: 16\n",
            "      pin_memory: True\n",
            "      drop_last: False\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cifar_kd.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}